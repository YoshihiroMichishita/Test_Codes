{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd02861481d7c14ccd7b8f974c16842ada1472ac646794883dd08fa4739b3fa7118",
   "display_name": "Python 3.8.5 64-bit ('anaconda3-2020.11')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 1. パーセプトロンと論理回路\n",
    "---\n",
    "以下のような条件によって0,1を出力する関数をパーセプトロンと呼ぶ。\n",
    "$$\n",
    "f(x,y) = \\begin{cases}\n",
    " 0 & (b + x\\omega_1 + y\\omega_2 \\leq 0)\\\\\n",
    "  1 & (b + x\\omega_1 + y\\omega_2 > 0)\n",
    "  \\end{cases}\n",
    "$$\n",
    "$\\omega_1,\\omega_2$を$x,y$の重み、$b$をバイアスと呼ぶ。\n",
    "\n",
    "論理回路AND,OR,NANDはパーセプトロンを用いて適切な$(b,\\omega_1,\\omega_2)$のもと表現することが出来る。\n",
    "\n",
    "| (x,y) | AND | OR | NAND |\n",
    "|:-----:|:-----:|:-----:|:-----:|\n",
    "| (0,0) | 0 | 0 | 1 |\n",
    "| (1,0) | 0 | 1 | 1 |\n",
    "| (0,1) | 0 | 1 | 1 |\n",
    "| (1,1) | 1 | 1 | 0 |\n",
    "|(b,$\\omega_1,\\omega_2$) | (-1.5,1,1) | (-0.5,1,1) | (1.5,-1,-1)|\n",
    "\n",
    "その他に重要な論理回路としてXORが上げられるが、XORは１層のパーセプトロンでは表現することが出来ない。\n",
    "代わりに２層のパーセプトロンによって、\n",
    "$$ XOR(x,y) = AND( NAND(x,y), OR(x,y) ) $$\n",
    "と表現することが出来る。\n",
    "\n",
    "論理回路てしてのパーセプトロンは例えばpythonを用いて以下のように書ける\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "source": [
    "def AND(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    w = np.array([1.0, 1.0])\n",
    "    b = -1.5\n",
    "    tmp = np.sum(w*x) + b\n",
    "    if tmp <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NAND(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    w = np.array([-1.0, -1.0])\n",
    "    b = 1.5\n",
    "    tmp = np.sum(w*x) + b\n",
    "    if tmp <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OR(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    w = np.array([1.0, 1.0])\n",
    "    b = -0.5\n",
    "    tmp = np.sum(w*x) + b\n",
    "    if tmp <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XOR(x1, x2):\n",
    "    s1 = NAND(x1, x2)\n",
    "    s2 = OR(x1, x2)\n",
    "    y = AND(s1, s2)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0, 1, 1]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "[AND(0,1), OR(0,1), XOR(0,1)]"
   ]
  },
  {
   "source": [
    "# 2. ニューラルネットワーク\n",
    "---\n",
    "\n",
    "ここまで考えてきたパーセプトロンは、実は単純パーセプトロンと呼ばれるもので、入出力は0,1であった。\n",
    "ここでは、より一般化したものとして、ニューラルネットワークを導入したい。以下のような一般化をする。\n",
    "- 入力を実数にする(複素数にするとどうなるか？)\n",
    "- $a=b+\\vec{w}\\cdot\\vec{x}$が0より大きいかで0,1を出力をしていたが(すなわち関数で表現するとステップ関数$\\theta(a)$を用いていたが)、別の関数系も用いることが出来る。一般的なのは、シグモイド関数($f(a)=1/(1+\\exp(-a))$)と、ReLU関数($f(a) = \\mathrm{max}(0,a)$)である。これらのaから出力を得る関数を活性化関数と呼ぶ\n",
    "- よって出力も一般の実数となる\n",
    "\n",
    "ここで一つ重要なのは、活性化関数は非線形な関数である必要があることである。例えば線形な関数$f(a)=pa+q$であった場合、ニューラルネットワークを多層にする意味がなくなってしまう。($f(f(a)) = p^2 a + q(p+1)$のように、１層のニューラルネットワークで表現できてしまう。)\n",
    "\n",
    "最後の出力層については、どのような問題を考えるかによって関数を変える。\n",
    "回帰問題といわれる、連続的な数値を求める問題については出力層は恒等式でそのまま与える。\n",
    "分類問題といわれる、与えられた場合のいずれに分類されるかを求める問題については、ソフトマックス関数を用いるのが一般的である。ソフトマックス関数は以下の式で表される。\n",
    "$$y_k = \\exp[a_k]/(\\sum_k \\exp[a_k])$$\n",
    "思想としては、微妙な分類の判定を際立たせるために指数関数の方に数値を載せて、また確率的な解釈をするために全体の和で規格化している。\n",
    "\n",
    "以下、pythonによる実装を考える。バイアスとして各層に常に１の値を持つニューラルネットワークを配置する。また次の層に移る際の重み付けを、行列を用いて表現すれば良い。\n",
    "まず活性化関数を実装する。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_fn(x):\n",
    "    return np.array(x>0, dtype=np.int)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a-c)\n",
    "    exp_sum = np.sum(exp_a)\n",
    "    y = exp_a / exp_sum\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network():\n",
    "    network = {}\n",
    "    network['W1'] = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])\n",
    "    network['b1'] = np.array([0.1,0.2,0.3])\n",
    "    network['W2'] = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])\n",
    "    network['b2'] = np.array([0.1,0.2])\n",
    "    network['W3'] = np.array([[0.1,0.3],[0.2,0.4]])\n",
    "    network['b3'] = np.array([0.1,0.2])\n",
    "\n",
    "    return network\n",
    "\n",
    "def forward(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = copy.copy(a3)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.31682708 0.69627909]\n"
     ]
    }
   ],
   "source": [
    "network = init_network()\n",
    "x = np.array([1.0,0.5])\n",
    "y = forward(network, x)\n",
    "print(y)"
   ]
  },
  {
   "source": [
    "# 3. ニューラルネットワークの学習\n",
    "----\n",
    "\n",
    "### 3.1. 損失関数\n",
    "ニューラルネットワークの予測の精度（の悪さ）を示す指標・関数を損失関数と呼ぶ。この損失関数の勾配を用いて、パラメータの更新を行い学習を推進していく。\n",
    "\n",
    "良く用いられるものは、2乗和誤差と交差エントロピー誤差である。それぞれ以下の式で表される。\n",
    "$$\n",
    "Err = \\begin{cases}\n",
    "\\frac{1}{2}\\sum_k(y_k-t_k)^2 & (\\mathrm{mean \\ square \\ error})\\\\\n",
    "-\\sum_k t_k \\mathrm{log}y_k & (\\mathrm{cross \\ entropy \\ error})\n",
    "\\end{cases}\n",
    "$$\n",
    "どちらも予測結果$y_k$が教師結果$t_k$に等しければ0になるようになっている。（正直もっと良い損失関数ありそう）\n",
    "\n",
    "以下pythonによる実装である。実際にはいくつかのデータにおける損失の平均をとってその精度を測定することになるので、そのように実装している。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y,t):\n",
    "    batch_size = y.shape[0]\n",
    "    return 0.5 * np.sum((y-t)**2) / batch_size\n",
    "\n",
    "def CEE(y,t):\n",
    "    delta = 0.00001\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + delta)) / batch_size\n",
    "\n",
    "def Grad(f, x):\n",
    "    dx = 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        temp = x[idx]\n",
    "        x[idx] = temp + dx\n",
    "        y1 = f(x)\n",
    "        x[idx] = temp - dx\n",
    "        y2 = f(x)\n",
    "        grad[idx] = (y1 -y2) / (2 * dx)\n",
    "        x[idx] = temp\n",
    "    \n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2層ニューラルネットワークのクラス\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return CEE(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        ya = np.argmax(y, axis=1)\n",
    "        ta = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = np.sum(y == t) / float(x.shape[0])\n",
    "        return acc\n",
    "\n",
    "    def Num_Grad(self, x, t):\n",
    "        def loss_W(W):\n",
    "            return self.loss(x,t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = Grad(loss_W, self.params['W1'])\n",
    "        grads['b1'] = Grad(loss_W, self.params['b1'])\n",
    "        grads['W2'] = Grad(loss_W, self.params['W2'])\n",
    "        grads['b2'] = Grad(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "mnist\n",
    "\n",
    "def load_mnist(normalize = False, flatten = False, one_hot_label = False):\n",
    "  (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "  if (normalize):\n",
    "    x_train = x_train / 255\n",
    "    x_test = x_test / 255\n",
    "\n",
    "  if (flatten):\n",
    "    x_train = x_train.reshape(len(x_train), -1)\n",
    "    x_test = x_test.reshape(len(x_test), -1)\n",
    "\n",
    "  if (one_hot_label):\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "  return (x_train,y_train), (x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(60000, 784)\n(60000, 10)\n(10000, 784)\n(10000, 10)\n(784, 50)\n(50,)\n(50, 10)\n(10,)\n39200\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 784 is out of bounds for axis 0 with size 784",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-faeb326a2410>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mt_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNum_Grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'W2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-4098167d17fb>\u001b[0m in \u001b[0;36mNum_Grad\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-c1994bf6eed5>\u001b[0m in \u001b[0;36mGrad\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 784 is out of bounds for axis 0 with size 784"
     ]
    }
   ],
   "source": [
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True,flatten=True, one_hot_label=True)\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "print(x_test.shape)\n",
    "print(t_test.shape)\n",
    "\n",
    "train_loss_list = []\n",
    "Ite = []\n",
    "\n",
    "iters_num = 1000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "print(network.params['W1'].shape)\n",
    "print(network.params['b1'].shape)\n",
    "print(network.params['W2'].shape)\n",
    "print(network.params['b2'].shape)\n",
    "print(network.params['W1'].size)\n",
    "\n",
    "\n",
    "for i in range(iters_num):\n",
    "\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.Num_Grad(x_batch, t_batch)\n",
    "\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    Ite.append(i)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(Ite, train_loss_list, label=\"loss\")\n",
    "plt.xlabel(\"Iterartion\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}